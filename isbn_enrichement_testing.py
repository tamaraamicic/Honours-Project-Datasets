# -*- coding: utf-8 -*-
"""ISBN Enrichement Testing.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UhAJEFeUeYRU0GXZ7nZjScWzFq3au8-m
"""

import pandas as pd
import requests
import time
import os
import re

items_info_cleaned_2_df = pd.read_csv('https://raw.githubusercontent.com/tamaraamicic/Honours-Project-Datasets/refs/heads/main/items_info_cleaned_2.dat', sep='\t', on_bad_lines='skip', index_col=False,)
items_info_cleaned_2_df.head()

import pandas as pd
import requests
import time
import os
import re

# --- CONFIGURATION ---
API_KEY = "59541_7b280637020d60a99abb89a246a1df18"
BASE_URL = "https://api2.isbndb.com/book/"
SAVE_FILE = "/content/isbn_enriched_data.csv"

headers = {"Authorization": API_KEY}

# --- Load dataset ---
url = 'https://raw.githubusercontent.com/tamaraamicic/Honours-Project-Datasets/refs/heads/main/items_info_cleaned_2.dat'
items_info_cleaned_2_df = pd.read_csv(url, sep='\t', on_bad_lines='skip', index_col=False)

# Extract ISBNs
isbn_list = items_info_cleaned_2_df['ISBN'].astype(str).unique().tolist()

# --- Function to validate ISBNs ---
def is_valid_isbn(isbn):
    """Checks if an ISBN is valid (either 10 or 13 digits, allowing 'X' at the end for ISBN-10)."""
    return bool(re.match(r'^\d{9}[\dX]$|^\d{13}$', isbn))

# --- Function to fetch book data ---
def fetch_book_data(isbn):
    try:
        url = f"{BASE_URL}{isbn}"
        response = requests.get(url, headers=headers)
        if response.status_code == 200:
            data = response.json().get('book', {})
            return {
                'isbn10': data.get('isbn10', ''),
                'isbn13': data.get('isbn13', ''),
                'subjects': "|".join(data.get('subjects', [])),  # Convert list to string
                'synopsis': data.get('synopsis', ''),  # May be missing for some books
                'language': data.get('language', ''),
                'pages': data.get('pages', '')
            }
        else:
            return None
    except Exception as e:
        print(f"Error fetching {isbn}: {e}")
        return None

# --- TEST: Fetch First 5 ISBNs Only ---
test_isbn_list = isbn_list[:5]  # Select only the first 5 ISBNs
test_valid_isbns = [isbn for isbn in test_isbn_list if is_valid_isbn(isbn)]

print(f"Testing with {len(test_valid_isbns)} valid ISBNs...")

# --- Run API calls for the first 5 valid ISBNs ---
test_results = []
for isbn in test_valid_isbns:
    book_info = fetch_book_data(isbn)
    if book_info:
        test_results.append(book_info)
    time.sleep(1)  # API rate limit

# Convert test results to a DataFrame and display
test_results_df = pd.DataFrame(test_results)
test_results_df.head()

import pandas as pd
import requests
import time
import os
import re

# --- CONFIGURATION ---
API_KEY = "59541_7b280637020d60a99abb89a246a1df18"
BASE_URL = "https://api2.isbndb.com/book/"
BATCH_SIZE = 10  # Since the API allows max 10 results per call
DAILY_LIMIT = 2000  # Academic plan limit
SAVE_FILE = "/content/isbn_enriched_data.csv"  # Where enriched data is saved
SKIPPED_FILE = "/content/skipped_isbns.csv"  # Log of invalid ISBNs

headers = {"Authorization": API_KEY}

# --- Load dataset ---
url = 'https://raw.githubusercontent.com/tamaraamicic/Honours-Project-Datasets/refs/heads/main/items_info_cleaned_2.dat'
items_info_cleaned_2_df = pd.read_csv(url, sep='\t', on_bad_lines='skip', index_col=False)

# Extract unique ISBNs
isbn_list = items_info_cleaned_2_df['ISBN'].astype(str).unique().tolist()

# --- Function to validate ISBNs ---
def is_valid_isbn(isbn):
    """Checks if an ISBN is valid (either 10 or 13 digits, allowing 'X' at the end for ISBN-10)."""
    return bool(re.match(r'^\d{9}[\dX]$|^\d{13}$', isbn))

# Filter valid and invalid ISBNs
valid_isbns = [isbn for isbn in isbn_list if is_valid_isbn(isbn)] # [:10] IF TRYING FOR FIRST 10
invalid_isbns = [isbn for isbn in isbn_list if not is_valid_isbn(isbn)]

# Save invalid ISBNs for reference
pd.DataFrame({'Invalid_ISBNs': invalid_isbns}).to_csv(SKIPPED_FILE, index=False)

print(f"Total valid ISBNs to process: {len(valid_isbns)}")
print(f"{len(invalid_isbns)} invalid ISBNs logged in {SKIPPED_FILE}.")

# --- Resume from saved progress ---
if os.path.exists(SAVE_FILE):
    enriched_df = pd.read_csv(SAVE_FILE)
    processed_isbns = set(enriched_df['isbn10'].astype(str)) | set(enriched_df['isbn13'].astype(str))
    print(f"Resuming from saved progress. {len(processed_isbns)} books already processed.")
else:
    enriched_df = pd.DataFrame(columns=['isbn10', 'isbn13', 'subjects', 'synopsis', 'language', 'pages'])
    processed_isbns = set()

# --- Function to fetch book data ---
def fetch_book_data(isbn):
    try:
        url = f"{BASE_URL}{isbn}"
        response = requests.get(url, headers=headers)

        if response.status_code == 200:
            data = response.json().get('book', {})
            return {
                'isbn10': data.get('isbn10', ''),
                'isbn13': data.get('isbn13', ''),
                'subjects': "|".join(data.get('subjects', [])),  # Convert list to string
                'synopsis': data.get('synopsis', ''),
                'language': data.get('language', ''),
                'pages': data.get('pages', '')
            }
        else:
            print(f"Error fetching {isbn}: {response.status_code} - {response.json()}")
            return None
    except Exception as e:
        print(f"Exception for {isbn}: {e}")
        return None

# --- Process in batches (Respecting API Limits) ---
total_calls = 0  # Track API calls

for i in range(0, len(valid_isbns), BATCH_SIZE):
    if total_calls >= DAILY_LIMIT:
        print("Daily API limit reached. Stopping for today.")
        break

    batch_isbns = valid_isbns[i:i + BATCH_SIZE]
    batch_results = []

    print(f"Processing batch {i // BATCH_SIZE + 1}/{len(valid_isbns) // BATCH_SIZE + 1}...")

    for isbn in batch_isbns:
        if isbn in processed_isbns:  # Skip already processed ISBNs
            continue

        book_info = fetch_book_data(isbn)
        if book_info:
            batch_results.append(book_info)
            processed_isbns.add(book_info['isbn10'])
            processed_isbns.add(book_info['isbn13'])
            total_calls += 1

        if total_calls >= DAILY_LIMIT:
            print("Daily API limit reached. Stopping for today.")
            break  # Stop processing if limit is reached

        time.sleep(1)  # Prevent hitting API rate limits

    # Convert batch results to DataFrame and append to CSV
    if batch_results:
        batch_df = pd.DataFrame(batch_results)
        enriched_df = pd.concat([enriched_df, batch_df], ignore_index=True)
        enriched_df.to_csv(SAVE_FILE, index=False)  # Save progress

    print(f"Batch {i // BATCH_SIZE + 1} completed. Progress saved.")

print("All valid ISBNs processed successfully or daily limit reached.")

# Review Skipped ISBNs
skipped_isbns_df = pd.read_csv("/content/skipped_isbns.csv")
skipped_isbns_df.head()

from google.colab import files

# Download the processed data
files.download("/content/isbn_enriched_data.csv")

# Download skipped ISBNs
files.download("/content/skipped_isbns.csv")

# Load enriched data
isbn_enriched_df = pd.read_csv(SAVE_FILE)

# Merge using ISBN-10 (since Book-Crossing uses ISBN-10)
merged_df = items_info_cleaned_2_df.merge(isbn_enriched_df, left_on='ISBN', right_on='isbn10', how='left')

# Drop redundant columns
merged_df = merged_df.drop(columns=['isbn10', 'isbn13'])

# Save the final enriched dataset
merged_df.to_csv("/content/Enhanced_Book_Crossing.csv", index=False)

# Download it
from google.colab import files
files.download("/content/Enhanced_Book_Crossing.csv")

"""This is where I start playing around with things to have a fully detailed dataset:"""

import pandas as pd
import requests
import time
import os
import re

# --- Load dataset ---
url1 = 'https://raw.githubusercontent.com/tamaraamicic/Honours-Project-Datasets/refs/heads/main/items_info_cleaned_2.dat'
items_info_cleaned_2_df = pd.read_csv(url1, sep='\t', on_bad_lines='skip', index_col=False)


url2 = 'https://raw.githubusercontent.com/tamaraamicic/Honours-Project-Datasets/refs/heads/main/isbn_enriched_data.csv'
items_enriched_data = pd.read_csv(url2, sep=',', on_bad_lines='skip', index_col=False)

items_info_cleaned_2_df.head()

items_enriched_data.head()

# Rename ISBN column in one of the datasets
items_info_cleaned_2_df.rename(columns={'ISBN': 'ISBN10'}, inplace=True)
items_enriched_data.rename(columns={'isbn10': 'ISBN10'}, inplace=True)

# Perform an inner merge so that only books with matching ISBNs in both datasets will be kept (we will only lose 24 books by doing this, see invalid ISBNs datatset on github is needed)
enriched_books_df = pd.merge(items_info_cleaned_2_df, items_enriched_data, on='ISBN10', how='inner')
enriched_books_df.head()

# Drop the images because I don't think we need this
enriched_books_df.drop(columns=['Image-URL-S', 'Image-URL-M', 'Image-URL-L'], inplace=True)
enriched_books_df.head()

# Check for NaN values in each column
empty_columns = enriched_books_df.isna().sum()
# Print columns with missing values
print(empty_columns[empty_columns > 0])

# Identify the rows where the 'subject' column has missing values (NaN)
missing_subject_rows = enriched_books_df[enriched_books_df['subjects'].isna()]

# Print out the columns for those rows
print("Columns with missing subjects:")
print(missing_subject_rows)

# Split the 'subject' column by the pipe (|) symbol
enriched_books_df['subjects'] = enriched_books_df['subjects'].str.split('|')

# Ensure any NaN values are replaced with an empty list so that they don't cause issues
enriched_books_df['subjects'] = enriched_books_df['subjects'].apply(lambda x: x if isinstance(x, list) else [])

# Flatten the lists in the 'subjects' column and count unique subjects
all_subjects = enriched_books_df['subjects'].explode().dropna()

# Count the unique subjects
unique_subject_count = all_subjects.nunique()

# Print the count of unique subjects
print(f"Number of unique subjects: {unique_subject_count}")

# Optional: If you want to print the actual unique subjects
unique_subjects = all_subjects.unique()
print(f"Unique subjects: {unique_subjects}")

# Create a list of unique subjects
df_subjects = unique_subjects

# Apply one-hot encoding for each unique subject
for subject in df_subjects:
    enriched_books_df[subject] = enriched_books_df['subjects'].apply(lambda x: 1 if subject in x else 0)

# Print the resulting DataFrame to check the changes
print(enriched_books_df.head())

# List of ISBNs with missing subjects
missing_isbns = [
    "0894803700", "0676974562", "0884092097", "9510236756", "0553802550",
    "8448902378", "1410798860", "0679777547", "8475060099", "848300268X",
    "0967231434", "0597145806", "8090128920", "0613572742", "0868065951",
    "8806143581", "0743455967", "030768993X", "059033560X", "0909486727",
    "0441783295", "0312141890", "0536597073", "0449219089", "0736625828",
    "0891904891", "8486542480", "0962994235", "0140084096", "0130980056",
    "0749318627", "1929365438", "3635605360", "0860097374", "0973100117",
    "0451167724", "0880882239", "9035110617", "0425177378", "1413708986",
    "0670032549", "979428386X", "0671832107", "061398546X", "0850917565",
    "0340190175", "9503703689", "0671817086", "1842120646", "8821162028",
    "2890452107", "8474440904", "8429805931", "9072766997", "0835619060",
    "0590393871"
]

# Manually add subjects for each ISBN (for demonstration purposes, we're using a dictionary of ISBN -> subject)
isbn_to_subject = {
    "0894803700": ["Happiness"],
    "0676974562": ["Love", "War", "Psychological fiction", "Romance - Contemporary"],
    "0884092097": ["Poetry", "Coming of Age"],
    "9510236756": ["Biography / Autobiography", "Cultural Studies"],
    "0553802550": ["Parenting & Families", "Parenting"],
    "8448902378": ["Adventure", "Teen & Young Adult"],
    "1410798860": ["World War II", "Historical", "Memoirs", "Parenting Girls", "Parenting"],
    "0679777547": ["Mystery, Thriller & Suspense", "Adventure"],
    "8475060099": ["Contemporary", "Spain"],
    "848300268X": ["Women Authors"],
    "0967231434": ["Writing", "Journal Writing"],
    "0597145806": ["Business"],
    "8090128920": ["Coming of Age", "First Loves", "Teen & Young Adult"],
    "0613572742": ["Eastern", "Spiritual", "Religion & Spirituality"],
    "0868065951": ["Poetry"],
    "8806143581": ["Short Stories", "Satire"],
    "0743455967": ["Memoirs", "Writing"],
    "030768993X": ["Children's Books"],
    "059033560X": ["Family Relationships", "Divorce", "Behavioral Psychology", "Siblings"],
    "0909486727": ["Australia", "Australia & Oceania"],
    "0441783295": ["Fiction / Science Fiction / General", "Science Fiction"],
    "0312141890": ["Children's Books", "Coming of Age"],
    "0536597073": ["Africa", "Geography & Cultures"],
    "0449219089": ["Crime"],
    "0736625828": ["Romance - Historical", "Historical Fiction"],
    "0891904891": ["Greek & Roman", "Fantasy"],
    "8486542480": ["Short Stories"],
    "0962994235": ["Health", "Medicine", "Alternative Medicine"],
    "0140084096": ["Australia", "Holocaust"],
    "0130980056": ["Marketing & Sales"],
    "0749318627": ["Biography / Autobiography", "Love & Loss"],
    "1929365438": ["Schools & Teaching"],
    "3635605360": ["Erotica", "Sexuality", "Dreams"],
    "0860097374": ["Love"],
    "0973100117": ["Family Relationships"],
    "0451167724": ["Personal Transformation", "Time Management"],
    "0880882239": ["Self-Help", "Journal Writing"],
    "9035110617": ["History - Military / War", "History"],
    "0425177378": ["Contemporary", "Romance", "Family Relationships"],
    "1413708986": ["Mechanical", "Engineering"],
    "0670032549": ["Science Fiction & Fantasy", "Horror"],
    "979428386X": ["Economics"],
    "0671832107": ["Fiction", "Paranormal"],
    "061398546X": ["Fiction"],
    "0850917565": ["Children's Books"],
    "0340190175": ["Children's Books"],
    "9503703689": ["Political", "Memoirs"],
    "0671817086": ["Politics & Social Sciences"],
    "1842120646": ["Politics & Government"],
    "8821162028": ["Italy", "Fiction"],
    "2890452107": ["Travelers & Explorers"],
    "8474440904": ["Music", "Composers & Musicians"],
    "8429805931": ["Short Stories"],
    "9072766997": ["Cultural"],
    "0835619060": ["Alternative Medicine", "Meditation"],
    "0590393871": ["Children's Books"]
}

# Loop through each ISBN in the missing list
for isbn in missing_isbns:
    # Get the list of subjects for this ISBN (this could have multiple subjects)
    subjects = isbn_to_subject.get(isbn)

    if subjects:
        # For each subject, set the value to 1 in the corresponding column
        for subject in subjects:
            if subject in enriched_books_df.columns:
                enriched_books_df.loc[enriched_books_df['ISBN10'] == isbn, subject] = 1
            else:
                print(f"Subject '{subject}' does not exist as a column for ISBN: {isbn}")

# Flatten the list of subjects for use as column names
subject_columns = list(set([subject for subjects in isbn_to_subject.values() for subject in subjects]))

# Verify the update (print the updated rows)
print(enriched_books_df.loc[enriched_books_df['ISBN10'].isin(missing_isbns), ['ISBN10', 'subjects'] + subject_columns])

# Check for NaN values in each column
empty_columns = enriched_books_df.isna().sum()
# Print columns with missing values
print(empty_columns[empty_columns > 0])

# Save the DataFrame to a CSV file
enriched_books_df.to_csv('/content/enriched_books_dataset.csv', index=False)

# Iterate over each subject column to count how many books have that subject (i.e., count the 1s)
subject_counts = enriched_books_df[df_subjects].sum()

# Sort the counts in descending order
sorted_subject_counts = subject_counts.sort_values(ascending=False)

# Print out each subject and its count, sorted from most to least common
for subject, count in sorted_subject_counts.items():
    print(f"Subject: {subject}, Count of books with this subject: {count}")